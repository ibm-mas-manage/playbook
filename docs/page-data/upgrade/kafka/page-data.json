{"componentChunkName":"component---src-pages-upgrade-kafka-mdx","path":"/upgrade/kafka/","result":{"pageContext":{"frontmatter":{"title":"Manage Integration using Kafka"},"relativePagePath":"/upgrade/kafka.mdx","titleType":"append","MdxNode":{"id":"e24670d7-94ff-5c8b-95a4-455e6ab5093b","children":[],"parent":"c7c9f56d-6576-5e7a-a59e-165969b477f9","internal":{"content":"---\ntitle: Manage Integration using Kafka\n---\n\nThe Maximo Integration Framework (MIF) supports asynchronous message processing using Kafka providers in a similar manner that the MIF can use JMS queues.  Kafka Topics persist messages to a data store for outbound Publish/Notification Channel messages and inbound Enterprise Service messages.  As well, Manage’s processing model for Kafka queues is similar to the processing model for JMS, where there is support for Sequential and Continuous message processing.\n\n## Manage and Kafka\nManage components provide support for using Kafka as a message queue for outbound (Publish Channel) and inbound (Enterprise Service) messages.  \n- Manage CRON tasks are used to consume messages from the queues.  \n- MIF file loading, Interface Table CRON task and Servlet are examples of components used to write (produce) messages to the queues.   \n- Within the External Systems application, there is an Action to support configuration of the Provider registration and queue processing.  \n- The Manage REST API provides support for browsing messages within the queue.\n\t- Note: there is no View/Delete Queue Messages feature for Kafka queues.\n\nKafka has a server configuration for the message size limit.  Consider the size of your transactions, especially when Attachments are integrated, when configuring this limit.  Be aware that the message size can impact the performance of reading/writing with the queue.\nManage provides configuration to set the maximum message size that the processing will handle when reading from and writing to Kafka.  The default message size is 10MB.  If you need to increase (or decrease) this size you can add this System Property mxe.kafka.messagesize, and set its value (in bytes) to be larger (or smaller) than 10MB.\nYou need to ensure that the Kafka server has a matching message size with what is used within Manage.\n\n## Kafka Message Processing with Manage\nKafka message processing keeps track of processed messages by maintaining an offset value (count) which represents the next message to be processed.  When a message is processed in Kafka, the offset is now beyond that message and that message will never be processed again.  The message is not deleted from the queue after being processed. It will remain in the queue until the configured expiration time is met for the message.  Be aware that Manage’s error processing, described below, will facilitate messages being re-processed when an error condition is encountered.\n\nIf you have messages in error that expire, the message is no longer in the queue, but it is still maintained in Manage’s Message Reprocessing application.  From here, the message can be manually re-processed as needed.  This re-processing option is only available via the application and there would be no automated re-processing of expired messages.\n\nIf you have messages in error that you never intend to re-process, you must configure the KafkaErrorsCleanup cron task to remove expired messages from Message Reprocessing.  These messages must be marked for deletion in Message Reprocessing before the Cron task will delete them. \n\nSequential queues don’t use an error queue since there is only one message in error at any time. A message in error will prevent other messages (behind the error) from processing. An error does not prevent new messages from being added to the queue.  Kafka retains messages in the queue (those processed and unprocessed) based on the retention/expiration time that is set in Kafka.  Setting this value properly is critical to ensure messages do not expire prior to being processed.  \n\nFor example, if you set the Kafka expiration to 48 hours and a message goes in error late Friday evening and other messages are stacked up behind the error.  If the error message is not addressed until Monday AM, the error message and other unprocessed messages may expire before messages continue processing.\n\nA reminder that messages that are in error and then expire, can still be re-processed from the Message Reprocessing application.\n\nA continuous queue **must have** an error queue configured.  A message that hits an error will be written to the bottom (end) of the queue so that it re-processes.  This will happen based on the Maximum Retry value (count) that is configured.  Once the message in error is processed up to the Maximum Retry count then it is written to the Error queue (status of HOLD) and Message re-processing and will no longer be processed in continuous queue. \n\nThe processing of the error queue is sequential and does facilitate automated re-processing of messages in error provided that the top message in the queue is successfully reprocessed or deleted.\n\nAgain, keep in mind if messages are sitting in a queue (sequential or continuous) but not being processed (i.e. cron task instance is not Active), there is a possibility that messages could expire before ever being processed.  In this case there is no recovery for those messages, since they didn’t process with an error, they would not appear in Message Reprocessing (contains only messages that hit an error).\n\n## Apache Kafka Providers\nYou can integrate with the Apache Kafka messaging platform to publish and consume messages asynchronously from the Kafka platform. Kafka can be configured as an external message provider that accepts and stores outbound and inbound integration messages.\n\nThe product provides the Kafka messaging client that supports consuming, producing, and browsing Kafka topics. To use Kafka features in the product, you must obtain and configure a Kafka server instance of the Kafka provider. You can choose any Kafka provider that is compatible with the Kafka 2.5 client libraries. \n\nThe Kafka provider manages the topics and the corresponding partitions and storage that persist the integration messages. The Maximo® integration framework provides the Kafka client framework to read, write, and browse messages in Kafka topics. \n\nThe integration framework only uses messaging features of Kafka and does not use any of the Kafka streaming features. \n\nThe framework uses the point-to-point delivery of Kafka messages. Thus, all Kafka topics are used as a messaging queue by the integration framework. Only one consumer group per topic is used to consume messages. A separate consumer group is used for browsing. Messages are not broadcast to multiple consumer groups. However, to enable browsing of topics, one other consumer group per topic is available.\n\nNote: (as of February 2021) there is a known Kafka bug on Windows that can cause a Kafka failure if a queue has no messages processed over a period of time that is beyond 72 hours.\n\nSpecifying Kafka providers for message processing\n\nYou can specify Kafka as a messaging provider to asynchronously process outbound and inbound integration framework messages. The integration framework supplies a Kafka client. \n\n### Before you begin\nObtain and configure a Kafka provider that is compatible with the Kafka 2.5 libraries. \n### About this task\nKafka refers to a topic as a category or feed name to which records are published. Because of the way that the integration framework works with Kafka, topics are referred to as messaging queues. \n### Procedure\n- In the External Systems application, add or select the external system and then select Add/Modify Message Providers to open the Message Hub Provider dialog box. \n- Insert a row and add a Kafka provider name. \nThe provider type is specified as KAFKA.\n- Specify values for the following properties. The SASL_MECHANISM and SECURITY_PROTOCOL properties are optional. \n\n<table>\n  <tr>\n    <th>Option</th>\n    <th>Description</th>\n  </tr>\n\n  <tr>\n    <td>SASL_MECHANISM</td>\n    <td>\n     <pre>Fully Supported.The mechanism that implements the Simple Authentication and Security Layer (SASL) authentication and security framework.</pre>\n     <pre>Allowed values are PLAIN, GSSAPI, SCRAM-SHA-256, and SCRAM-SHA-512.</pre>\n     <p></p>\n     <pre>If no value is specified, the default value of PLAIN is used.</pre>\n   </td> \n  </tr>\n\t  \n  <tr>\n    <td>SECURITY_PROTOCOL</td>\n    <td>\n     <pre>The protocol that ensures secure data sharing. Allowed values are SASL_SSL, SSL, PLAINTEXT, or SASL_PLAINTEXT.</pre>\n   </td> \n  </tr>   \n\n   <tr>\n    <td>BOOTSTRAPSERVERS\n    <p></p>\n    <p></p>\n    PASSWORD\n    <p></p>\n    USERNAME</td>\n    <td>\n     <pre>A comma-separated list of server URLs that host your Kafka instance. Include the port specification. For example, specify brokerurl1,brokerurl2.</pre> \n     <pre>You can use quotation marks if needed.</pre>\n     <p></p>\n     <pre>If you specify the security protocol as either SASL_SSL or SASL_PLAINTEXT, specify an authentication password.</pre>\n     <p></p>\n     <pre>The username for your password.</pre>\n   </td> \n  </tr>   \n\n  </table>  \n\n- Save your changes.\n\n### What to do next\n- Register the Kafka topics as Manage queues in the External Systems application. \n- For continuous inbound queues, you must configure a corresponding sequential error queues or risk having messages expire.\n- Configure one or more cron tasks to poll your queues and process the queue messages.\n\n### Registering Kafka topics as Manage queues\n\nTo exchange message data and contents with Kafka, you register Kafka topics as Manage message queues. \n\nTo exchange message data and contents with Kafka, you register Kafka topics as Manage® message queues. \n\nBefore you can register Kafka topics as product queues, you must take the following steps: \n- Create the topics on the Kafka server. \n- Configure one or more message providers of the Kafka type in the External Systems application.\n- If you plan to use an error queue, create it in Kafka, and then in the product, register the error queue. For example, you can register an error queue for a continuous inbound queue as a sequential inbound error queue.\n\n### Procedure\n1. In the External Systems application, select the system for which you want to add a queue. \n2. Select Add/Modify Queues and add a queue. \n3. Specify the Kafka provider and topic name. Specify the message direction and whether the queue is sequential or continuous. \nAll outbound queues are sequential queues. You specify a queue as continuous by leaving the Sequential check box deselected and specify a queue as outbound by leaving the Inbound check box deselected.\n4.\tOptional: For continuous queues, you should specify an error queue. \nYou specify an error queue in your Kafka instance and then register the error queue as a sequential inbound queue. Since it is a sequential queue, error processing stops until the failed message either expires from Kafka or is fixed and retried.\nIf you do not specify an error queue, the framework tries to process a failed message according to the value that is specified for the retry count on the queue. If the message still fails, processing moves on to the next message offset and an administrator needs to process the message in the Message Reprocessing application. \n5.\tSpecify the email address of the user who receives notifications when errors occur in the queue. This email address is used instead of the email value that is specified in system properties. \n6.\tIn the Retention Time field, specify the time in hours for the message to be held in the MAXINTERROR table. Set this time to match the time that is specified for this topic in Kafka. The Kafka configuration pertains to the expiration of the message in the queue.\nAfter the retention time is exceeded, messages are deleted by the KafkaErrorsCleanup cron task if they both are expired from Kafka and are marked for deletion in the Message Reprocessing application. The product does not control message expiration in Kafka.\n7.\tIn the Maximum Retry Count field, specify the maximum number of reprocessing attempts after an initial error and before a message is put in a Hold status and moved to the error queue (if an error queue is configured) for an administrator to review.  A recommended value to set would be 5, but your integration requirements may determine a different value.  \n8.\tAs mentioned above the Kafka continuous consumer writes the message to the bottom of the queue with each error retry and continuously scrolls forward. \n9.\tSave your changes. \n### What to do next\nFor each queue, add an instance of the Kafka cron task to periodically poll the queue for messages and process them. Configure a Kafka error cleanup cron task (described further down in this document) to delete the appropriate messages from the error table. \n### Outbound sequential message processing\nThe integration framework uses Kafka topics as outbound, sequential message queues. When a message is sent from the integration framework, it is routed to your Kafka provider, where it is hosted as a JSON message in a predefined topic. Although publish channels can send messages in XML or JSON format, the product wraps them as JSON formatted messages and writes them to the Kafka topic. An outbound message uses the structure that is shown in the following code example: \n```\n{\n      “interfacetype”:”MAXIMO”,\n      “INTERFACE”:”MXPRInterface”,\n      “payload”:”<xml/json message>”,\n      “SENDER”:”MX”,\n      “destination”:”<external system name>”,\n      “destjndiname”:”<kafka topic name>”,\n      “compressed”:”1”,\n      “MEAMessageID”:”<providername>~<topic>~<partition>~<offset>”,\n      “mimetype”:”application/xml”\n}\n```\n\nThe payload property contains the payload that is sent to the endpoint. The entire Kafka message is  compressed at time of storage inside the Kafka partitions. In the Kafka partitions, messages are stored in a strict timestamp order. The position of the message in the partition is given a sequential ID number that is known as a message offset. Although Kafka topics can be partitioned, sequentially processed topics must be in a single partition. \n\nAfter the message is written to the Kafka topic, it is processed by the associated Kafka cron task that you configure in the Cron Task Setup application for each topic. Processing occurs according to the external system and endpoint that are configured for the Channel. \n\nIf you need to consume messages directly from a queue, not using the MIF CRON process to deliver messages, then you must de-compress (Java) the message as part of the consumption process.\n\n### Inbound sequential processing\nInbound message processing can be sequential or continuous. Inbound sequential processing is similar to outbound sequential processing and requires the same types of configuration, including an instance of the Kafka cron task. \nThe structure of a sequential inbound message is shown in the following example: \n\n```\n{\n     “interfacetype”: \"MAXIMO\",\n     “SENDER”: \"testkafka\",\n     “destination”: \"testkafka\",\n     “USER”: \"wilson\",\n     “MEAMessageID”: \"….\",\n     “INTERFACE”: \"MXASSETInterface\",\n     “payload”: \"{\"assetnum\":\"AZ163\",\"siteid\":\"BEDFORD\"}\",\n     “mimetype”: \"application/json\",\n     “destjndiname”: \"anamitratestcont\"\n}\n```\n\nIf you write messages directly to a queue, not using MIF components, then you must create the message following the above schema. You must provide a compressed (Java) message.\n\nA sequential queue should be configured with a single partition and have one (and only one) Cron task instance to consume messages from the queue.\n### Error processing for sequential queues\nWith sequential queues, the first message that fails after the retry count that is specified for the queue, the Kafka cron task stops and does not move to the next message until the failed message either is corrected or marked for deletion in the Message Reprocessing application, or the message expires in Kafka. \n\nExpired messages can be corrected and retried manually, but they are not removed from the error table automatically. You must configure the KafkaErrorsCleanup cron task to remove expired messages after the specified retention time if they are marked for deletion. Set the Manage's retention time, when registering the queue, to match the retention time that is set in Kafka.\n### Continuous Queue processing\nInbound continuous message processing with Kafka is similar to JMS continuous message, however there are some differences in scaling and in error processing. In general, continuous queue processing will not stop processing when a message hits an error (as compared to a sequential queue). Messages in error are copied and pushed to an Error queue once they are processed the number of times as determined using the Retry Count.\n\nA single Kafka queue can be scaled to have multiple consumers (multi-threaded) of messages by defining additional partitions for the queue.  Each partition would represent a separate consumer and each partition requires its own Cron Task instance to support message consumption for the partition. To scale processing, multiple partitions should be configured and the number of Cron task instances should match the number of partitions.\n\nAnother scaling option is that you can deploy multiple Kafka queues, with one or multiple partitions.  For example, you may set up separate queues for each external system that you integrate with.\n### Error processing for continuous inbound queues\nFor continuous inbound processing, you MUST set up an error queue in Kafka, register it in the product as a sequential inbound queue, and specify it as the error queue for a continuous inbound queue. You need to specify a Kafka CRON task instance to process messages from the error queue. \n\nWhen you configure a continuous queue in the External Systems application, you specify a message Retry count and a retention time (in hours). The retry count value determines how many times the Kafka cron task for the continuous queue attempts to reprocess the message before the message is moved to the error queue. When an error is hit for a message in the continuous queue, the Kafka consumer rewrites the message to the bottom (end) of the queue with each error retry. If new messages are written to the queue before the rewrite occurs, those new messages are processed before the rewritten ‘errored’ message. Because of this processing, errors may be resolved if a new message provides data in Manage that is needed by the previously failed message.\n\nMessages that fail processing (and hit the retry limit) are moved to the configured error queue in a HOLD status. Your configured error queue is a sequential queue, where the first failed message in the error queue stops the processing of the error queue until the failed message is fixed and retried or the message expires in Kafka.\n\nFailing to configure an error queue for your Continuous queue could result in messages never being processed.\n\n### Configuration of a redelivery delay for continuous queues\nAn administrator can configure a redelivery delay (default is 5 seconds) that works when a retry count is specified for a continuous queue. When a redelivery delay is specified in the mxe.int.kafkaredeldelay property, the cron task for the queue does not rewrite a message to the front of the Kafka queue until after the delay time. The delay time is specified in seconds. Note:  this property is not provided and must be created if there is a need to implement a delay.\n\nFor example, if the delay is set to 30 seconds, the cron task for the continuous queue pauses for 30 seconds before it rewrites the message with errors to the front of the queue. The delay allows other messages to be processed in the queue, some of which can contain objects that are needed to successfully process the message that had errors. The drawback of using this mechanism is that the delay slows down the processor because it delays the cron task from continued processing. \n\n- Cron tasks for Kafka message consumption\n  - You configure an instance of the Kafka cron task for each of your Kafka queues. The cron task consumes Kafka messages based on the schedule that you specify. All message consumption is controlled by the Kafka cron task.\n- You configure an instance of the Kafka cron task for each of your Kafka queues. The cron task consumes Kafka messages based on the schedule that you specify. All message consumption is controlled by the Kafka cron task.\n- Kafka cron task\n- The cron task consumes messages according to whether the queue is sequential or continuous. In the Cron Task Setup application, you filter for the Kafka cron task. Add one or more instances of the Kafka cron task and define its schedule, the administrative user to notify about problems, whether the cron is active, and whether to keep history. For each instance, specify values for the following properties:\n\n<table>\n  <tr>\n    <th>Property</th>\n    <th>Description</th>\n    <th>Default</th>\n  </tr>\n\n  <tr>\n    <td>TOPIC</td>\n    <td>The name of the Kafka topic. This value is required.</td> \n    <td>None</td>\n  </tr>\n\t  \n  <tr>\n    <td>MESSAGEHUB</td>\n    <td>The name of the Kafka provider that is registered in the product. This value is required.</td> \n    <td>None</td>\n  </tr>\n\n  <tr>\n    <td>MESSAGEPROCESSOR</td>\n    <td>The message processor. The same message processor is used for JMS processing. This value is optional. If no value is specified, the default values are used. </td> \n    <td>\n     <pre>Inbound: psdi.iface.jms.QueueToMaximoProcessor</pre>\n     <pre>Outbound: psdi.iface.jms.QueueToDestManagerProcessor</pre>\n     <pre>Notification: com.ibm.tivoli.maximo.notification.NotificationMessageProcessor</pre>\n   </td>\n  </tr>\n\n</table>  \n\n\n### Browsing the queue\nManage provides a REST api that will let you browse the content of messages in a Kafka queue.  You will need a browser session or some other API tool to execute a REST request.\n\nThe API format is shown in the following examples \n\n- In this example you need to provide your Manage environment’s Hostname:Port and the Kafka Provider and Topic names\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?&lean=1\n- This example shows the use of the pageSize to limit the content in the response\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?oslc.pageSize=100\n- This example shows the use of the ‘all’ parameter to show all records or records from the offset point forward.  Default value is 0 (false) which shows messages from the offset forward.\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?all=1\n    - A value of 1 will show all records, ignoring the offset value.\n- This example shows the use of the pageSize to limit the content in the response and identifies the Partition\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name>/Kafka Topic Name?oslc.pageSize=100&partition=0\n- This example shows the use of the pageSize to limit the content in the response and identifies the Partition and requests and 2nd page of data\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?oslc.pageSize=100&partition=0&pageno=2\n\nKeep in mind when you are viewing a queue, you will see messages that are unprocessed as well as those that have been processed and have not yet expired.  In a Continuous queue you could see multiple iterations of a message if it encountered an error during processing. \n\n### KafkaErrorsCleanup Cron Task\nKafka does not provide a way to delete messages from the server. When a message is older than the retention time that you configure on the server, Kafka expires the message. Manage® cron tasks cannot process expired message from the queue, and if any of the expired messages didn't process because of errors, they remain in the Message Reprocessing application and can be managed from there.\n\nThe KafkaErrorsCleanup cron task can be configured to clean up the Kafka expired messages that meet the following conditions: \n- The message must be older than the retention time in hours that is specified when you add a message queue for a publish channel or enterprise service or notification channel. The integration administrator should set this time to match the retention time that is specified when the topic is registered in Kafka. \n- The message must be marked for deletion in the product or already processed. Messages that are already processed are no longer visible in the Message Reprocessing application, but they are deleted by the cleanup cron task. \n\nMessages that have a status of Hold or Retry are not deleted by the cron task. The integration administrator can try to reprocess those messages manually from the Message Reprocessing application or mark the message for deletion.\n","type":"Mdx","contentDigest":"4c738cc7bc50e69b20cf9b1d0b0713b9","owner":"gatsby-plugin-mdx","counter":149},"frontmatter":{"title":"Manage Integration using Kafka"},"exports":{},"rawBody":"---\ntitle: Manage Integration using Kafka\n---\n\nThe Maximo Integration Framework (MIF) supports asynchronous message processing using Kafka providers in a similar manner that the MIF can use JMS queues.  Kafka Topics persist messages to a data store for outbound Publish/Notification Channel messages and inbound Enterprise Service messages.  As well, Manage’s processing model for Kafka queues is similar to the processing model for JMS, where there is support for Sequential and Continuous message processing.\n\n## Manage and Kafka\nManage components provide support for using Kafka as a message queue for outbound (Publish Channel) and inbound (Enterprise Service) messages.  \n- Manage CRON tasks are used to consume messages from the queues.  \n- MIF file loading, Interface Table CRON task and Servlet are examples of components used to write (produce) messages to the queues.   \n- Within the External Systems application, there is an Action to support configuration of the Provider registration and queue processing.  \n- The Manage REST API provides support for browsing messages within the queue.\n\t- Note: there is no View/Delete Queue Messages feature for Kafka queues.\n\nKafka has a server configuration for the message size limit.  Consider the size of your transactions, especially when Attachments are integrated, when configuring this limit.  Be aware that the message size can impact the performance of reading/writing with the queue.\nManage provides configuration to set the maximum message size that the processing will handle when reading from and writing to Kafka.  The default message size is 10MB.  If you need to increase (or decrease) this size you can add this System Property mxe.kafka.messagesize, and set its value (in bytes) to be larger (or smaller) than 10MB.\nYou need to ensure that the Kafka server has a matching message size with what is used within Manage.\n\n## Kafka Message Processing with Manage\nKafka message processing keeps track of processed messages by maintaining an offset value (count) which represents the next message to be processed.  When a message is processed in Kafka, the offset is now beyond that message and that message will never be processed again.  The message is not deleted from the queue after being processed. It will remain in the queue until the configured expiration time is met for the message.  Be aware that Manage’s error processing, described below, will facilitate messages being re-processed when an error condition is encountered.\n\nIf you have messages in error that expire, the message is no longer in the queue, but it is still maintained in Manage’s Message Reprocessing application.  From here, the message can be manually re-processed as needed.  This re-processing option is only available via the application and there would be no automated re-processing of expired messages.\n\nIf you have messages in error that you never intend to re-process, you must configure the KafkaErrorsCleanup cron task to remove expired messages from Message Reprocessing.  These messages must be marked for deletion in Message Reprocessing before the Cron task will delete them. \n\nSequential queues don’t use an error queue since there is only one message in error at any time. A message in error will prevent other messages (behind the error) from processing. An error does not prevent new messages from being added to the queue.  Kafka retains messages in the queue (those processed and unprocessed) based on the retention/expiration time that is set in Kafka.  Setting this value properly is critical to ensure messages do not expire prior to being processed.  \n\nFor example, if you set the Kafka expiration to 48 hours and a message goes in error late Friday evening and other messages are stacked up behind the error.  If the error message is not addressed until Monday AM, the error message and other unprocessed messages may expire before messages continue processing.\n\nA reminder that messages that are in error and then expire, can still be re-processed from the Message Reprocessing application.\n\nA continuous queue **must have** an error queue configured.  A message that hits an error will be written to the bottom (end) of the queue so that it re-processes.  This will happen based on the Maximum Retry value (count) that is configured.  Once the message in error is processed up to the Maximum Retry count then it is written to the Error queue (status of HOLD) and Message re-processing and will no longer be processed in continuous queue. \n\nThe processing of the error queue is sequential and does facilitate automated re-processing of messages in error provided that the top message in the queue is successfully reprocessed or deleted.\n\nAgain, keep in mind if messages are sitting in a queue (sequential or continuous) but not being processed (i.e. cron task instance is not Active), there is a possibility that messages could expire before ever being processed.  In this case there is no recovery for those messages, since they didn’t process with an error, they would not appear in Message Reprocessing (contains only messages that hit an error).\n\n## Apache Kafka Providers\nYou can integrate with the Apache Kafka messaging platform to publish and consume messages asynchronously from the Kafka platform. Kafka can be configured as an external message provider that accepts and stores outbound and inbound integration messages.\n\nThe product provides the Kafka messaging client that supports consuming, producing, and browsing Kafka topics. To use Kafka features in the product, you must obtain and configure a Kafka server instance of the Kafka provider. You can choose any Kafka provider that is compatible with the Kafka 2.5 client libraries. \n\nThe Kafka provider manages the topics and the corresponding partitions and storage that persist the integration messages. The Maximo® integration framework provides the Kafka client framework to read, write, and browse messages in Kafka topics. \n\nThe integration framework only uses messaging features of Kafka and does not use any of the Kafka streaming features. \n\nThe framework uses the point-to-point delivery of Kafka messages. Thus, all Kafka topics are used as a messaging queue by the integration framework. Only one consumer group per topic is used to consume messages. A separate consumer group is used for browsing. Messages are not broadcast to multiple consumer groups. However, to enable browsing of topics, one other consumer group per topic is available.\n\nNote: (as of February 2021) there is a known Kafka bug on Windows that can cause a Kafka failure if a queue has no messages processed over a period of time that is beyond 72 hours.\n\nSpecifying Kafka providers for message processing\n\nYou can specify Kafka as a messaging provider to asynchronously process outbound and inbound integration framework messages. The integration framework supplies a Kafka client. \n\n### Before you begin\nObtain and configure a Kafka provider that is compatible with the Kafka 2.5 libraries. \n### About this task\nKafka refers to a topic as a category or feed name to which records are published. Because of the way that the integration framework works with Kafka, topics are referred to as messaging queues. \n### Procedure\n- In the External Systems application, add or select the external system and then select Add/Modify Message Providers to open the Message Hub Provider dialog box. \n- Insert a row and add a Kafka provider name. \nThe provider type is specified as KAFKA.\n- Specify values for the following properties. The SASL_MECHANISM and SECURITY_PROTOCOL properties are optional. \n\n<table>\n  <tr>\n    <th>Option</th>\n    <th>Description</th>\n  </tr>\n\n  <tr>\n    <td>SASL_MECHANISM</td>\n    <td>\n     <pre>Fully Supported.The mechanism that implements the Simple Authentication and Security Layer (SASL) authentication and security framework.</pre>\n     <pre>Allowed values are PLAIN, GSSAPI, SCRAM-SHA-256, and SCRAM-SHA-512.</pre>\n     <p></p>\n     <pre>If no value is specified, the default value of PLAIN is used.</pre>\n   </td> \n  </tr>\n\t  \n  <tr>\n    <td>SECURITY_PROTOCOL</td>\n    <td>\n     <pre>The protocol that ensures secure data sharing. Allowed values are SASL_SSL, SSL, PLAINTEXT, or SASL_PLAINTEXT.</pre>\n   </td> \n  </tr>   \n\n   <tr>\n    <td>BOOTSTRAPSERVERS\n    <p></p>\n    <p></p>\n    PASSWORD\n    <p></p>\n    USERNAME</td>\n    <td>\n     <pre>A comma-separated list of server URLs that host your Kafka instance. Include the port specification. For example, specify brokerurl1,brokerurl2.</pre> \n     <pre>You can use quotation marks if needed.</pre>\n     <p></p>\n     <pre>If you specify the security protocol as either SASL_SSL or SASL_PLAINTEXT, specify an authentication password.</pre>\n     <p></p>\n     <pre>The username for your password.</pre>\n   </td> \n  </tr>   \n\n  </table>  \n\n- Save your changes.\n\n### What to do next\n- Register the Kafka topics as Manage queues in the External Systems application. \n- For continuous inbound queues, you must configure a corresponding sequential error queues or risk having messages expire.\n- Configure one or more cron tasks to poll your queues and process the queue messages.\n\n### Registering Kafka topics as Manage queues\n\nTo exchange message data and contents with Kafka, you register Kafka topics as Manage message queues. \n\nTo exchange message data and contents with Kafka, you register Kafka topics as Manage® message queues. \n\nBefore you can register Kafka topics as product queues, you must take the following steps: \n- Create the topics on the Kafka server. \n- Configure one or more message providers of the Kafka type in the External Systems application.\n- If you plan to use an error queue, create it in Kafka, and then in the product, register the error queue. For example, you can register an error queue for a continuous inbound queue as a sequential inbound error queue.\n\n### Procedure\n1. In the External Systems application, select the system for which you want to add a queue. \n2. Select Add/Modify Queues and add a queue. \n3. Specify the Kafka provider and topic name. Specify the message direction and whether the queue is sequential or continuous. \nAll outbound queues are sequential queues. You specify a queue as continuous by leaving the Sequential check box deselected and specify a queue as outbound by leaving the Inbound check box deselected.\n4.\tOptional: For continuous queues, you should specify an error queue. \nYou specify an error queue in your Kafka instance and then register the error queue as a sequential inbound queue. Since it is a sequential queue, error processing stops until the failed message either expires from Kafka or is fixed and retried.\nIf you do not specify an error queue, the framework tries to process a failed message according to the value that is specified for the retry count on the queue. If the message still fails, processing moves on to the next message offset and an administrator needs to process the message in the Message Reprocessing application. \n5.\tSpecify the email address of the user who receives notifications when errors occur in the queue. This email address is used instead of the email value that is specified in system properties. \n6.\tIn the Retention Time field, specify the time in hours for the message to be held in the MAXINTERROR table. Set this time to match the time that is specified for this topic in Kafka. The Kafka configuration pertains to the expiration of the message in the queue.\nAfter the retention time is exceeded, messages are deleted by the KafkaErrorsCleanup cron task if they both are expired from Kafka and are marked for deletion in the Message Reprocessing application. The product does not control message expiration in Kafka.\n7.\tIn the Maximum Retry Count field, specify the maximum number of reprocessing attempts after an initial error and before a message is put in a Hold status and moved to the error queue (if an error queue is configured) for an administrator to review.  A recommended value to set would be 5, but your integration requirements may determine a different value.  \n8.\tAs mentioned above the Kafka continuous consumer writes the message to the bottom of the queue with each error retry and continuously scrolls forward. \n9.\tSave your changes. \n### What to do next\nFor each queue, add an instance of the Kafka cron task to periodically poll the queue for messages and process them. Configure a Kafka error cleanup cron task (described further down in this document) to delete the appropriate messages from the error table. \n### Outbound sequential message processing\nThe integration framework uses Kafka topics as outbound, sequential message queues. When a message is sent from the integration framework, it is routed to your Kafka provider, where it is hosted as a JSON message in a predefined topic. Although publish channels can send messages in XML or JSON format, the product wraps them as JSON formatted messages and writes them to the Kafka topic. An outbound message uses the structure that is shown in the following code example: \n```\n{\n      “interfacetype”:”MAXIMO”,\n      “INTERFACE”:”MXPRInterface”,\n      “payload”:”<xml/json message>”,\n      “SENDER”:”MX”,\n      “destination”:”<external system name>”,\n      “destjndiname”:”<kafka topic name>”,\n      “compressed”:”1”,\n      “MEAMessageID”:”<providername>~<topic>~<partition>~<offset>”,\n      “mimetype”:”application/xml”\n}\n```\n\nThe payload property contains the payload that is sent to the endpoint. The entire Kafka message is  compressed at time of storage inside the Kafka partitions. In the Kafka partitions, messages are stored in a strict timestamp order. The position of the message in the partition is given a sequential ID number that is known as a message offset. Although Kafka topics can be partitioned, sequentially processed topics must be in a single partition. \n\nAfter the message is written to the Kafka topic, it is processed by the associated Kafka cron task that you configure in the Cron Task Setup application for each topic. Processing occurs according to the external system and endpoint that are configured for the Channel. \n\nIf you need to consume messages directly from a queue, not using the MIF CRON process to deliver messages, then you must de-compress (Java) the message as part of the consumption process.\n\n### Inbound sequential processing\nInbound message processing can be sequential or continuous. Inbound sequential processing is similar to outbound sequential processing and requires the same types of configuration, including an instance of the Kafka cron task. \nThe structure of a sequential inbound message is shown in the following example: \n\n```\n{\n     “interfacetype”: \"MAXIMO\",\n     “SENDER”: \"testkafka\",\n     “destination”: \"testkafka\",\n     “USER”: \"wilson\",\n     “MEAMessageID”: \"….\",\n     “INTERFACE”: \"MXASSETInterface\",\n     “payload”: \"{\"assetnum\":\"AZ163\",\"siteid\":\"BEDFORD\"}\",\n     “mimetype”: \"application/json\",\n     “destjndiname”: \"anamitratestcont\"\n}\n```\n\nIf you write messages directly to a queue, not using MIF components, then you must create the message following the above schema. You must provide a compressed (Java) message.\n\nA sequential queue should be configured with a single partition and have one (and only one) Cron task instance to consume messages from the queue.\n### Error processing for sequential queues\nWith sequential queues, the first message that fails after the retry count that is specified for the queue, the Kafka cron task stops and does not move to the next message until the failed message either is corrected or marked for deletion in the Message Reprocessing application, or the message expires in Kafka. \n\nExpired messages can be corrected and retried manually, but they are not removed from the error table automatically. You must configure the KafkaErrorsCleanup cron task to remove expired messages after the specified retention time if they are marked for deletion. Set the Manage's retention time, when registering the queue, to match the retention time that is set in Kafka.\n### Continuous Queue processing\nInbound continuous message processing with Kafka is similar to JMS continuous message, however there are some differences in scaling and in error processing. In general, continuous queue processing will not stop processing when a message hits an error (as compared to a sequential queue). Messages in error are copied and pushed to an Error queue once they are processed the number of times as determined using the Retry Count.\n\nA single Kafka queue can be scaled to have multiple consumers (multi-threaded) of messages by defining additional partitions for the queue.  Each partition would represent a separate consumer and each partition requires its own Cron Task instance to support message consumption for the partition. To scale processing, multiple partitions should be configured and the number of Cron task instances should match the number of partitions.\n\nAnother scaling option is that you can deploy multiple Kafka queues, with one or multiple partitions.  For example, you may set up separate queues for each external system that you integrate with.\n### Error processing for continuous inbound queues\nFor continuous inbound processing, you MUST set up an error queue in Kafka, register it in the product as a sequential inbound queue, and specify it as the error queue for a continuous inbound queue. You need to specify a Kafka CRON task instance to process messages from the error queue. \n\nWhen you configure a continuous queue in the External Systems application, you specify a message Retry count and a retention time (in hours). The retry count value determines how many times the Kafka cron task for the continuous queue attempts to reprocess the message before the message is moved to the error queue. When an error is hit for a message in the continuous queue, the Kafka consumer rewrites the message to the bottom (end) of the queue with each error retry. If new messages are written to the queue before the rewrite occurs, those new messages are processed before the rewritten ‘errored’ message. Because of this processing, errors may be resolved if a new message provides data in Manage that is needed by the previously failed message.\n\nMessages that fail processing (and hit the retry limit) are moved to the configured error queue in a HOLD status. Your configured error queue is a sequential queue, where the first failed message in the error queue stops the processing of the error queue until the failed message is fixed and retried or the message expires in Kafka.\n\nFailing to configure an error queue for your Continuous queue could result in messages never being processed.\n\n### Configuration of a redelivery delay for continuous queues\nAn administrator can configure a redelivery delay (default is 5 seconds) that works when a retry count is specified for a continuous queue. When a redelivery delay is specified in the mxe.int.kafkaredeldelay property, the cron task for the queue does not rewrite a message to the front of the Kafka queue until after the delay time. The delay time is specified in seconds. Note:  this property is not provided and must be created if there is a need to implement a delay.\n\nFor example, if the delay is set to 30 seconds, the cron task for the continuous queue pauses for 30 seconds before it rewrites the message with errors to the front of the queue. The delay allows other messages to be processed in the queue, some of which can contain objects that are needed to successfully process the message that had errors. The drawback of using this mechanism is that the delay slows down the processor because it delays the cron task from continued processing. \n\n- Cron tasks for Kafka message consumption\n  - You configure an instance of the Kafka cron task for each of your Kafka queues. The cron task consumes Kafka messages based on the schedule that you specify. All message consumption is controlled by the Kafka cron task.\n- You configure an instance of the Kafka cron task for each of your Kafka queues. The cron task consumes Kafka messages based on the schedule that you specify. All message consumption is controlled by the Kafka cron task.\n- Kafka cron task\n- The cron task consumes messages according to whether the queue is sequential or continuous. In the Cron Task Setup application, you filter for the Kafka cron task. Add one or more instances of the Kafka cron task and define its schedule, the administrative user to notify about problems, whether the cron is active, and whether to keep history. For each instance, specify values for the following properties:\n\n<table>\n  <tr>\n    <th>Property</th>\n    <th>Description</th>\n    <th>Default</th>\n  </tr>\n\n  <tr>\n    <td>TOPIC</td>\n    <td>The name of the Kafka topic. This value is required.</td> \n    <td>None</td>\n  </tr>\n\t  \n  <tr>\n    <td>MESSAGEHUB</td>\n    <td>The name of the Kafka provider that is registered in the product. This value is required.</td> \n    <td>None</td>\n  </tr>\n\n  <tr>\n    <td>MESSAGEPROCESSOR</td>\n    <td>The message processor. The same message processor is used for JMS processing. This value is optional. If no value is specified, the default values are used. </td> \n    <td>\n     <pre>Inbound: psdi.iface.jms.QueueToMaximoProcessor</pre>\n     <pre>Outbound: psdi.iface.jms.QueueToDestManagerProcessor</pre>\n     <pre>Notification: com.ibm.tivoli.maximo.notification.NotificationMessageProcessor</pre>\n   </td>\n  </tr>\n\n</table>  \n\n\n### Browsing the queue\nManage provides a REST api that will let you browse the content of messages in a Kafka queue.  You will need a browser session or some other API tool to execute a REST request.\n\nThe API format is shown in the following examples \n\n- In this example you need to provide your Manage environment’s Hostname:Port and the Kafka Provider and Topic names\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?&lean=1\n- This example shows the use of the pageSize to limit the content in the response\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?oslc.pageSize=100\n- This example shows the use of the ‘all’ parameter to show all records or records from the offset point forward.  Default value is 0 (false) which shows messages from the offset forward.\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?all=1\n    - A value of 1 will show all records, ignoring the offset value.\n- This example shows the use of the pageSize to limit the content in the response and identifies the Partition\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name>/Kafka Topic Name?oslc.pageSize=100&partition=0\n- This example shows the use of the pageSize to limit the content in the response and identifies the Partition and requests and 2nd page of data\n  - http://localhost:7001/maximo/oslc/browsemsghub/Kafka Provider Name/Kafka Topic Name?oslc.pageSize=100&partition=0&pageno=2\n\nKeep in mind when you are viewing a queue, you will see messages that are unprocessed as well as those that have been processed and have not yet expired.  In a Continuous queue you could see multiple iterations of a message if it encountered an error during processing. \n\n### KafkaErrorsCleanup Cron Task\nKafka does not provide a way to delete messages from the server. When a message is older than the retention time that you configure on the server, Kafka expires the message. Manage® cron tasks cannot process expired message from the queue, and if any of the expired messages didn't process because of errors, they remain in the Message Reprocessing application and can be managed from there.\n\nThe KafkaErrorsCleanup cron task can be configured to clean up the Kafka expired messages that meet the following conditions: \n- The message must be older than the retention time in hours that is specified when you add a message queue for a publish channel or enterprise service or notification channel. The integration administrator should set this time to match the retention time that is specified when the topic is registered in Kafka. \n- The message must be marked for deletion in the product or already processed. Messages that are already processed are no longer visible in the Message Reprocessing application, but they are deleted by the cleanup cron task. \n\nMessages that have a status of Hold or Retry are not deleted by the cron task. The integration administrator can try to reprocess those messages manually from the Message Reprocessing application or mark the message for deletion.\n","fileAbsolutePath":"/home/travis/build/maximo/manage-playbook/src/pages/upgrade/kafka.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}